import streamlit as st
import time
from pypdf import PdfWriter
from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.base import TaskResult
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.azure import AzureAIChatCompletionClient
from azure.core.credentials import AzureKeyCredential
from autogen_core import CancellationToken
from autogen_core.tools import FunctionTool
from autogen_agentchat.messages import TextMessage
import asyncio

from elasticsearch import Elasticsearch
from search_duckduck import get_asearch
from search_elasticsearch import hybrid_search

from sentence_transformers import SentenceTransformer
import torch
torch.classes.__path__ = [] # add this line to manually set it to empty. 

    
def setup_page():
    st.set_page_config(
        page_title="	üê±‚Äçüèç Supter Chatbot",
        layout="centered"
    )
    st.header("Agentic Chatbot!" )
    st.sidebar.header("Options", divider='rainbow')
    hide_menu_style = """
            <style>
            #MainMenu {visibility: hidden;}
            </style>
            """ 
    st.markdown(hide_menu_style, unsafe_allow_html=True)        # Xo√° menu ch·ªçn m·∫∑c ƒë·ªãnh g√≥c tr√™n b√™n ph·∫£i c·ªßa st

def get_choice():
    choice = st.sidebar.radio("Choose:", ["Normal Chat",
                                          "Chat with a PDF",
                                          "Chat with many PDFs",
                                          "Chat with an image",
                                          "Chat with audio",
                                          "Chat with video"],)
    return choice

def get_clear():
    clear_button=st.sidebar.button("Start new session", key="clear")
    return clear_button

def reload_achat():
    if "messages" not in st.session_state:
            st.session_state.messages = []
    # Hi·ªÉn th·ªã l·∫°i l·ªãch s·ª≠ chat sau khi app ƒë∆∞·ª£c t·∫£i l·∫°i
    for message in st.session_state.messages:
        with st.chat_message(message['role'],  avatar="ü§ì" if message['role'] == "assistant" else "ü§¨"):
            st.markdown(message['content'])

def effect_chat(response):
    print("KAKAKA: ", response)
     # T·∫°o m·ªôt placeholder ƒë·ªÉ hi·ªÉn th·ªã ph·∫£n h·ªìi t·ª´ng t·ª´ m·ªôt
    message_placeholder = st.empty()
    response_text = ""
    for word in response.split():
        response_text += word + " "
        message_placeholder.markdown(response_text)
        time.sleep(0.05)  # Hi·ªáu ·ª©ng g√µ ch·ªØ

async def get_rag_prompt(query: str):
    retrieval_context = await hybrid_search(search_query=query, top_k=3, model=model)
    # Augment the query with the retrieval context.
    augmented_query = f"""Th√¥ng tin ƒë√£ ƒë∆∞·ª£c x√°c th·ª±c:\n{retrieval_context}.\n\n 
    C√¢u h·ªèi c·ªßa t√¥i l√†: {query}.\n\n ∆Øu ti√™n d·ª±a v√†o th√¥ng tin ƒë∆∞·ª£c x√°c th·ª±c, h√£y cung c·∫•p c√¢u tr·∫£ l·ªùi. 
    N·∫øu trong ph·∫°m vi th√¥ng tin ƒë√£ x√°c th·ª±c ch∆∞a ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi th√¨ m·ªõi d√πng th√¥ng tin b√™n ngo√†i kh√°c."""
    return augmented_query

async def get_response(prompt, chat_agent, user_agent):
    print("prompt: ", prompt)
    new_promt = await get_rag_prompt(prompt)
    print("NEW prompt: ", new_promt)
    termination = TextMentionTermination("APPROVE")
    team = RoundRobinGroupChat([chat_agent, user_agent], termination_condition=termination)
    prev_response = None 
    last_response = None 
    async for message in team.run_stream(task=new_promt):  # D√πng new_promt thay v√¨ prompt
        if isinstance(message, TaskResult):
            print("Stop Reason:", message.stop_reason)
            break
        else:
            print(message)
            prev_response = last_response  
            last_response = message.content  
    return prev_response if prev_response is not None else "No prior response"

def chat_processing(prompt, chat_agent, user_agent):
    with st.chat_message(name="user", avatar="ü§¨"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message(name="assistant", avatar="ü§ì"):
        response = asyncio.run(get_response(prompt, chat_agent, user_agent))
        effect_chat(response)
    st.session_state.messages.append({"role": "assistant", "content": response})

def main():
    choice = get_choice()
    if choice == "Normal Chat":
        # st.subheader("Normal Chat")
        clear = get_clear()
        if clear:
            if "messages" in st.session_state:
                del st.session_state.messages
        reload_achat()

        chat_agent = AssistantAgent(
                        name="tma_assistant",
                        model_client=client,
                        tools=[get_newinfo2TMA, get_realinfo2TMA],
                        description="M·ªôt nh√¢n vi√™n l·ªÖ t√¢n chuy√™n nghi·ªáp c·ªßa c√¥ng ty TMA",
                        system_message="""B·∫°n l√† m·ªôt nh√¢n vi√™n l·ªÖ t√¢n t·∫°i c√¥ng ty TMA v·ªõi phong c√°ch tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch·ªâ t·∫≠p trung tr·∫£ l·ªùi nh·ªØng g√¨ ƒë∆∞·ª£c h·ªèi.
                                        M·ª•c ti√™u c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√°c th√¥ng tin li√™n quan ƒë·∫øn c√¥ng ty TMA.
                                        B·∫°n ch·ªâ t·∫≠p trung l√†m vi·ªác v√†o m·ª•c ti√™u c·ªßa m√¨nh.
                                        ƒê·ª´ng t·ªën th·ªùi gian b·∫±ng tin nh·∫Øn chit chat.
                                        Xem x√©t c√°c ƒë·ªÅ xu·∫•t khi ƒë∆∞a ra 1 √Ω t∆∞·ªüng.
                                        ƒê·ª´ng c·ªë t·∫°o c√¢u tr·∫£ l·ªùi sai tr·ªçng t√¢m c√¢u h·ªèi.
                                        """,
                    )
        user_agent = AssistantAgent(
                        name="assistant",
                        model_client=client,
                        tools=[],
                        description="M·ªôt ki·ªÉm tra gi√∫p h·ªá th·ªëng ƒë√°nh gi√° xem c√¢u tr·∫£ l·ªùi c√≥ ph√π h·ª£p v·ªõi c√¢u h·ªèi ch∆∞a",
                        system_message="""B·∫°n c√≥ vai tr√≤ ki·ªÉm tra xem c√¢u tr·∫£ l·ªùi c·ªßa h·ªá th·ªëng c√≥ ƒë√∫ng v·ªõi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·∫∑t ch∆∞a.
                                        M·ª•c ti√™u c·ªßa b·∫°n ch·ªâ l√† xem x√©t c√¢u tr·∫£ l·ªùi c·ªßa h·ªá th√¥ng c√≥ ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng hay ch∆∞a. 
                                        N·∫øu th·∫•y c√¢u tr·∫£ l·ªùi c·ªßa h·ªá th·ªëng qu√° d√†i th√¨ h√£y nh·∫Øc h·ªá th·ªëng c√≥ th·ªÉ t√≥m t·∫Øt c√¢u tr·∫£ l·ªùi t·∫≠p trung v√†o n·ªôi dung c√¢u h·ªèi ƒë∆∞·ª£c hay kh√¥ng, m√† kh√¥ng c·∫ßn ƒë∆∞a ra v√≠ d·ª• c·ª• th·ªÉ cho h·ªá th·ªëng bi·∫øt.
                                        N·∫øu h·ªá th·ªëng ƒë√£ tr·∫£ l·ªùi l√† kh√¥ng bi·∫øt hay 'Hi·ªán nay, ch∆∞a c√≥ th√¥ng tin ch√≠nh th·ª©c v·ªÅ c√¢u h·ªèi n√†y' ho·∫∑c t∆∞∆°ng t·ª± th√¨ c≈©ng ph·∫£n h·ªìi b·∫±ng 'APPROVE'.
                                        N·∫øu c√¢u tr·∫£ l·ªùi c·ªßa h·ªá th·ªëng v√† c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·∫∑t ch∆∞a ph√π h·ª£p ho√†n to√†n th√¨ m·ªõi th√¥ng b√°o cho h·ªá th·ªëng tr·∫£ l·ªùi l√† 'Hi·ªán nay, ch∆∞a c√≥ th√¥ng tin ch√≠nh th·ª©c v·ªÅ c√¢u h·ªèi n√†y'. R·ªìi c≈©ng ph·∫£n h·ªìi b·∫±ng 'APPROVE'.
                                        B·∫°n ch·ªâ t·∫≠p trung l√†m vi·ªác v√†o m·ª•c ti√™u c·ªßa m√¨nh.
                                        ƒê·ª´ng t·ªën th·ªùi gian b·∫±ng tin nh·∫Øn ngo√†i l·ªÅ kh√°c.
                                        ƒê·ª´ng c·ªë t·∫°o c√¢u tr·∫£ l·ªùi sai tr·ªçng t√¢m c√¢u h·ªèi.
                                        """,
                    )
       
        if prompt := st.chat_input("nh·∫≠p c√¢u h·ªèi v√†o ƒë√¢y..."):
           chat_processing(prompt, chat_agent, user_agent)

# H√†m b·ªçc
async def hybrid_search_tool_func(*, search_query: str):
    return await hybrid_search(search_query=search_query, top_k=4, model=model)

if __name__ == "__main__":
    get_newinfo2TMA = FunctionTool(
        get_asearch, description="T√¨m ki·∫øm c√°c th√¥ng tin v·ªÅ c√¥ng ty TMA v√† c√°c th√¥ng tin li√™n quan kh√°c."
    )
    get_realinfo2TMA = FunctionTool(  
        hybrid_search_tool_func, description="T√¨m ki·∫øm c√°c th√¥ng tin v·ªÅ c√¥ng ty TMA."
    )

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    #VietNamese embedding: https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2?library=sentence-transformers
    model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2").to(device)
    
    GITHUB_TOKEN = ""
    client = AzureAIChatCompletionClient(
        # model="gpt-4o-mini",
        # model='gpt-4o',
        # model='Codestral-2501',
        model='Codestral-2501',
        endpoint="https://models.inference.ai.azure.com",
        credential=AzureKeyCredential(GITHUB_TOKEN),
        model_info={
            "json_output": True,
            "function_calling": True,
            "vision": False,
            "family": "unknown",
        },
    )

    setup_page()
    main()




